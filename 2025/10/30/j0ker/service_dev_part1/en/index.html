

<!DOCTYPE html>
<html lang="ko-KR" data-default-color-scheme="&#34;auto&#34;">



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.ico">
  <link rel="icon" type="image/png" href="/img/favicon.ico">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/thumbnail.jpg&#34; alt=&#34;index_img&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;0-What-happened&amp;#x2026;&#34;&gt;&lt;a href=&#34;#0-What-happened&amp;#x2026;&#34; class=&#34;headerlink&#34; title=&#34;0. What happened&amp;#x2026;&#34;&gt;&lt;/a&gt;0. What happened&amp;#x2026;&lt;/h1&gt;&lt;p&gt;Haha, it&amp;#x2019;s j0ker! Why&amp;#x2019;s it been so long since I last wrote? When was my last post&amp;#x2026; Ah, August&amp;#x2026; Haha&amp;#x2026; Please save me&amp;#x2026; Next time I&amp;#x2019;ll try to write something more interesting&amp;#x2026;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/joker.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Today, unlike my previous research posts, I&amp;#x2019;d like to briefly discuss a project I developed.&lt;/p&gt;
&lt;p&gt;On August 30th, the 31st Hacking Camp took place. About two weeks prior, I suddenly received a call from the organizers&amp;#x2026;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/kakao.png&#34; alt=&#34;&amp;#x1112;&amp;#x1162;&amp;#x110F;&amp;#x1175;&amp;#x11BC;&amp;#x110F;&amp;#x1162;&amp;#x11B7;&amp;#x1111;&amp;#x1173; &amp;#x1109;&amp;#x1165;&amp;#x11B8;&amp;#x110B;&amp;#x116C;.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Well&amp;#x2026; the original presenter suddenly contacted us saying they couldn&amp;#x2019;t make it, so we had to find a replacement. I was planning to go on the second day, listen to the mentee presentations, and then grab drinks with the organizers in the evening&amp;#x2026; but I ended up giving the presentation lol. So I prepared for it and had about a week to spare&amp;#x2026; but I&amp;#x2019;m a pro at creating work for myself, so I wanted to do something extra.&lt;/p&gt;
&lt;h1 id=&#34;1-Planning-Begins&#34;&gt;&lt;a href=&#34;#1-Planning-Begins&#34; class=&#34;headerlink&#34; title=&#34;1. Planning Begins&#34;&gt;&lt;/a&gt;1. Planning Begins&lt;/h1&gt;&lt;p&gt;To give a quick update, I recently co-founded a startup called L0ch and Hacky-AI, where we&amp;#x2019;re developing security solutions. (If you&amp;#x2019;re interested, please visit our &lt;a href=&#34;https://hacky-ai.com/&#34;&gt;website&lt;/a&gt;!) I believe the most crucial aspect of running a startup is identifying customer problems and proposing solutions. There are plenty of other things to worry about too&amp;#x2026; Anyway, that&amp;#x2019;s why I thought, &amp;#x201C;Why not create something simple that people attending presentations need?&amp;#x201D;&lt;/p&gt;
&lt;p&gt;Since I&amp;#x2019;m also building solutions, I&amp;#x2019;ve become very interested in development using AI services. At this point, the things I wanted to stick to as much as possible were:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Don&amp;#x2019;t write the code myself; let the AI services write it all.&lt;/li&gt;
&lt;li&gt;We only have a week, so just make it work.&lt;/li&gt;
&lt;li&gt;Build a service people actually need.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Hacking Camp runs a CTF on the first evening. My assumption was that most participants were just starting out with hacking and probably weren&amp;#x2019;t very familiar with reverse engineering yet! So, wouldn&amp;#x2019;t it be great to develop a reverse engineering assistant tool? I figured I could probably build something like that by investing about three days on my own. (But this was arrogance.) More specifically, I thought I could use an LLM to rename variables and functions to more readable names, and even analyze function contents with an LLM to add comments. (It&amp;#x2019;s been over a year since I last opened IDA, so I realized while building this that such tools already exist as open source&amp;#x2026; haha. Well, the point was for me to try building a service anyway.)&lt;/p&gt;
&lt;p&gt;A simplified service flowchart would look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;hljs less&#34;&gt;&amp;#x250C;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2510;                &amp;#x250C;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2510;                &amp;#x250C;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2510;
&amp;#x2502;   &lt;span class=&#34;hljs-selector-tag&#34;&gt;IDA&lt;/span&gt; &lt;span class=&#34;hljs-selector-tag&#34;&gt;Pro&lt;/span&gt;       &amp;#x2502;&amp;#x25C4;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x25BA;&amp;#x2502;   &lt;span class=&#34;hljs-selector-tag&#34;&gt;FastAPI&lt;/span&gt;       &amp;#x2502;&amp;#x25C4;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x25BA;&amp;#x2502;   &lt;span class=&#34;hljs-selector-tag&#34;&gt;Ollama&lt;/span&gt;        &amp;#x2502;
&amp;#x2502;   &amp;#xD50C;&amp;#xB7EC;&amp;#xADF8;&amp;#xC778;        &amp;#x2502;                &amp;#x2502;   &amp;#xC11C;&amp;#xBC84;           &amp;#x2502;                &amp;#x2502;   &lt;span class=&#34;hljs-selector-tag&#34;&gt;LLM&lt;/span&gt; &amp;#xC11C;&amp;#xBC84;       &amp;#x2502;
&amp;#x2502; (Python)        &amp;#x2502;                &amp;#x2502; (Python)        &amp;#x2502;                &amp;#x2502;                 &amp;#x2502;
&amp;#x2514;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2518;                &amp;#x2514;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2518;                &amp;#x2514;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2518;
                                            &amp;#x2502;                                   &amp;#x2502;
                                            &amp;#x25BC;                                   &amp;#x25BC;
                                    &amp;#x250C;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2510;               &amp;#x250C;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2510; 
                                    &amp;#x2502;   &lt;span class=&#34;hljs-selector-tag&#34;&gt;Supabase&lt;/span&gt;      &amp;#x2502;               &amp;#x2502;   &lt;span class=&#34;hljs-selector-tag&#34;&gt;Langfuse&lt;/span&gt;      &amp;#x2502;
                                    &amp;#x2502;   &amp;#xB370;&amp;#xC774;&amp;#xD130;&amp;#xBCA0;&amp;#xC774;&amp;#xC2A4;     &amp;#x2502;               &amp;#x2502;   &amp;#xBAA8;&amp;#xB2C8;&amp;#xD130;&amp;#xB9C1;         &amp;#x2502;
                                    &amp;#x2514;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2518;               &amp;#x2514;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2518;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;2-Ollama-Server&#34;&gt;&lt;a href=&#34;#2-Ollama-Server&#34; class=&#34;headerlink&#34; title=&#34;2. Ollama Server&#34;&gt;&lt;/a&gt;2. Ollama Server&lt;/h1&gt;&lt;p&gt;First, you might wonder why I&amp;#x2019;m not using a commercial service. After all, there are so many great models like Gemini and ChatGPT. My personal view is that &amp;#x201C;a small open-source model is sufficient for this service.&amp;#x201D; Before starting this venture, I used open-source models for projects at my previous company. Even now, while conducting research, I&amp;#x2019;ve tested various open-source models to build an on-premises environment. Unfortunately, performance limitations prevent me from testing larger models yet &amp;#x3160;&amp;#x3160;&lt;/p&gt;
&lt;p&gt;The device I currently use at home for LLM inference is a Mac mini. It has an M4 Pro chip and 64GB of RAM. I set up an Ollama server on it and served models. If you visit the &lt;a href=&#34;https://ollama.com/search&#34;&gt;Ollama homepage&lt;/a&gt;, you&amp;#x2019;ll find many models that can be utilized within 64GB of RAM.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/01.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-28 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 3.55.01.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Among the most popular models like GPT-OSS and DeepSeek, I tend to prefer models around 32B in size that are quantized to Q8. To give a simple reason:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;While 70B Q4 models (about 40G when downloaded) can run on 64GB RAM, these 70B models were already somewhat outdated at the time. I also thought it would be better to use a smaller model, considering the need to reserve space for the context window when processing long functions.&lt;/li&gt;
&lt;li&gt;Around mid-August, some hot new models emerged that were smaller yet performed well. For gpt-oss, there was a 20B MoE model, and qwen3 had 32B and 30B MoE models. These models seemed to deliver performance comparable to the latest ChatGPT 4o or slightly better.&lt;/li&gt;
&lt;li&gt;Beyond these, I also tested smaller models like Phi4 14B and Gemma 27B using several pre-set prompts I had prepared to evaluate their performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After multiple tests, I decided to use the &amp;#x201C;qwen3:30b-a3b-instruct-2507-q8_0&amp;#x201D; model.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;As expected, the MoE model is significantly faster at inference than the standard model. The Qwen3 32B q8 model achieved about 7 TPS, while the Qwen 30B MoE model reached up to 45 TPS. I felt this was sufficient for a small-scale service.&lt;/p&gt;
 &lt;pre&gt;&lt;code class=&#34;hljs yaml&#34;&gt;&lt;span class=&#34;hljs-attr&#34;&gt;total duration:&lt;/span&gt;       &lt;span class=&#34;hljs-number&#34;&gt;54.&lt;/span&gt;&lt;span class=&#34;hljs-string&#34;&gt;782917709s&lt;/span&gt;
&lt;span class=&#34;hljs-attr&#34;&gt;load duration:&lt;/span&gt;        &lt;span class=&#34;hljs-number&#34;&gt;83.&lt;/span&gt;&lt;span class=&#34;hljs-string&#34;&gt;491792ms&lt;/span&gt;
&lt;span class=&#34;hljs-attr&#34;&gt;prompt eval count:&lt;/span&gt;    &lt;span class=&#34;hljs-number&#34;&gt;2649 &lt;/span&gt;&lt;span class=&#34;hljs-string&#34;&gt;token(s)&lt;/span&gt;
&lt;span class=&#34;hljs-attr&#34;&gt;prompt eval duration:&lt;/span&gt; &lt;span class=&#34;hljs-number&#34;&gt;4.&lt;/span&gt;&lt;span class=&#34;hljs-string&#34;&gt;376536583s&lt;/span&gt;
&lt;span class=&#34;hljs-attr&#34;&gt;prompt eval rate:&lt;/span&gt;     &lt;span class=&#34;hljs-number&#34;&gt;605.27&lt;/span&gt; &lt;span class=&#34;hljs-string&#34;&gt;tokens/s&lt;/span&gt;
&lt;span class=&#34;hljs-attr&#34;&gt;eval count:&lt;/span&gt;           &lt;span class=&#34;hljs-number&#34;&gt;2293 &lt;/span&gt;&lt;span class=&#34;hljs-string&#34;&gt;token(s)&lt;/span&gt;
&lt;span class=&#34;hljs-attr&#34;&gt;eval duration:&lt;/span&gt;        &lt;span class=&#34;hljs-number&#34;&gt;49.&lt;/span&gt;&lt;span class=&#34;hljs-string&#34;&gt;986260996s&lt;/span&gt;
&lt;span class=&#34;hljs-attr&#34;&gt;eval rate:&lt;/span&gt;            &lt;span class=&#34;hljs-number&#34;&gt;45.87&lt;/span&gt; &lt;span class=&#34;hljs-string&#34;&gt;tokens/s&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The reason we did not use the reasoning model qwen3:30b-a3b-thinking-2507 was that a bug where reasoning failed to complete consistently appeared during prompt testing. Since the model had only recently been released at the time, we simply decided not to use it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;The reason for not using other Reasoning models was that they took significantly longer to complete outputs. While the instruct model could produce most outputs within 2 minutes, enabling Reasoning added an extra 1-2 minutes. Additionally, performance was sufficient even without Reasoning.&lt;/li&gt;
&lt;li&gt;Using smaller models required additional work, meaning their single-batch analysis performance was inferior. For the Phi4 14B model, while performance was sufficient, using the FP16 model was necessary to achieve usable performance levels. However, the reasoning time was too long for practical use. Beyond this, we determined that utilizing models like Qwen3 14B, 8B, Gemma 27B, and Deepseek 14B effectively would require significantly more prompt engineering effort.&lt;/li&gt;
&lt;li&gt;Considering various factors like speed, performance, and memory, we determined that using the &amp;#x201C;qwen3:30b-a3b-instruct-2507-q8_0&amp;#x201D; model was the most suitable choice.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With the model selected, simply set the environment variables and run the server using the &lt;code&gt;ollama serve&lt;/code&gt; command&amp;#x2014;that&amp;#x2019;s all it takes to get ready!&lt;/p&gt;
&lt;h1 id=&#34;2-Prompt-Logging-with-Langfuse&#34;&gt;&lt;a href=&#34;#2-Prompt-Logging-with-Langfuse&#34; class=&#34;headerlink&#34; title=&#34;2. Prompt Logging with Langfuse&#34;&gt;&lt;/a&gt;2. Prompt Logging with Langfuse&lt;/h1&gt;&lt;p&gt;You might be thinking, &amp;#x201C;Can&amp;#x2019;t we just use LangSmith?&amp;#x201D; I agree, but&amp;#x2026; since we tested it during the solution development process while considering on-premises deployment, I decided to give Langfuse a try this time.&lt;/p&gt;
&lt;p&gt;Installing Langfuse on the server isn&amp;#x2019;t difficult. Just grab it from the repository and deploy it via Docker. Then, register Langfuse&amp;#x2019;s callback handler in Langchain, and request logging will start as shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/02.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 4.22.19.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Logging allows you to verify whether responses to requests were sent correctly and check the quality of those responses. Based on this information, you can refine prompts or execution chains.&lt;/p&gt;
&lt;p&gt;And! You can also detect incoming attacks lol&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/03.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 4.24.45.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/04.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 4.24.58.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As shown above, we also detected prompt injection attacks being launched. You worked hard until dawn&amp;#x2026; Unfortunately, it seems you weren&amp;#x2019;t able to obtain the information you wanted.&lt;/p&gt;
&lt;h1 id=&#34;3-Lovable-Supabase&#34;&gt;&lt;a href=&#34;#3-Lovable-Supabase&#34; class=&#34;headerlink&#34; title=&#34;3. Lovable + Supabase&#34;&gt;&lt;/a&gt;3. Lovable + Supabase&lt;/h1&gt;&lt;p&gt;Originally, our website was developed using Base44. Yes, that&amp;#x2019;s the service &lt;a href=&#34;https://techcrunch.com/2025/06/18/6-month-old-solo-owned-vibe-coder-base44-sells-to-wix-for-80m-cash/&#34;&gt;acquired by Wix for $80M a few months ago&lt;/a&gt;. I tried it after reading this article too. It was fine for developing simple landing pages, but problems started piling up after adding login functionality. It was especially terrible when developing features that utilized the database.&lt;/p&gt;
&lt;p&gt;So, while searching for alternatives at the time, I heard that &amp;#x201C;Lovable is a hot service these days&amp;#x201D; and decided to give it a try. I first cloned the webpage I had previously developed exactly as it was, then started developing the login feature. To store user information, I connected it to Supabase. &lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/05.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 5.29.09.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Unlike Base44, it integrates directly with Supabase, makes database management easier, and seems to have many advantages overall. Of course, it might struggle to handle large-scale customers (my wallet, that is). Once you set up this integration and create the login button&amp;#x2026;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/06.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 5.36.58.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/07.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 5.37.39.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Yeah&amp;#x2026; it wasn&amp;#x2019;t exactly easy, but I did manage to succeed after a few tries. After logging in, there should be a dashboard, right? I&amp;#x2019;ll create it by writing the prompt well. &lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/08.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 5.43.31.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Of course, it doesn&amp;#x2019;t get cleanly developed with just that one prompt. If it had worked in one go, I&amp;#x2019;d be out buying Lovable stock right now.&lt;/p&gt;
&lt;p&gt;Anyway, there were still plenty of issues. While it generates API keys randomly just fine, it saves them to Supabase with parts of the key masked (&lt;em&gt;*&lt;/em&gt;), or arbitrarily removes buttons, and so on&amp;#x2014;there were a bunch of minor details to worry about. So I decided to narrow the scope of the features to implement. Originally, I wanted to add fun elements like a weekly Top 5 usage list&amp;#x2026; but it required more prompting than expected, so I decided to skip it for now.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/09.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 6.55.28.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;After about four hours of hard work, I managed to implement all the essential features. If I had developed it myself with my limited web development skills, it would have taken a week. Using AI services these days is definitely more advantageous.&lt;/p&gt;
&lt;p&gt;I&amp;#x2019;ll wrap things up here for today. Next time, I&amp;#x2019;ll cover the rest of the development process and what happened at the presentation site. Until then&amp;#x2026; wish me luck staying alive&amp;#x2026; Bye!&lt;/p&gt;
 - hack &amp; life">
  <meta name="author" content="j0dev, y2sman">
  <meta name="keywords" content>
  <meta name="google-site-verification" content="DXkyiaX95-ws53Tyt0m91_umRf4gfV2qJIQZ5zQDIO4">
  <meta name="naver-site-verification" content="0b4fea742ed293b82621684e466d9f26c3ccee06">


  <meta property="og:type" content="website"> 
  <meta property="og:title" content="[Research] Newbie Developer&#39;s Service Development Journey using AI Services Part 1 - hackyboiz">
  <meta property="og:description" content="&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/thumbnail.jpg&#34; alt=&#34;index_img&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;0-What-happened&amp;#x2026;&#34;&gt;&lt;a href=&#34;#0-What-happened&amp;#x2026;&#34; class=&#34;headerlink&#34; title=&#34;0. What happened&amp;#x2026;&#34;&gt;&lt;/a&gt;0. What happened&amp;#x2026;&lt;/h1&gt;&lt;p&gt;Haha, it&amp;#x2019;s j0ker! Why&amp;#x2019;s it been so long since I last wrote? When was my last post&amp;#x2026; Ah, August&amp;#x2026; Haha&amp;#x2026; Please save me&amp;#x2026; Next time I&amp;#x2019;ll try to write something more interesting&amp;#x2026;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/joker.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Today, unlike my previous research posts, I&amp;#x2019;d like to briefly discuss a project I developed.&lt;/p&gt;
&lt;p&gt;On August 30th, the 31st Hacking Camp took place. About two weeks prior, I suddenly received a call from the organizers&amp;#x2026;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/kakao.png&#34; alt=&#34;&amp;#x1112;&amp;#x1162;&amp;#x110F;&amp;#x1175;&amp;#x11BC;&amp;#x110F;&amp;#x1162;&amp;#x11B7;&amp;#x1111;&amp;#x1173; &amp;#x1109;&amp;#x1165;&amp;#x11B8;&amp;#x110B;&amp;#x116C;.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Well&amp;#x2026; the original presenter suddenly contacted us saying they couldn&amp;#x2019;t make it, so we had to find a replacement. I was planning to go on the second day, listen to the mentee presentations, and then grab drinks with the organizers in the evening&amp;#x2026; but I ended up giving the presentation lol. So I prepared for it and had about a week to spare&amp;#x2026; but I&amp;#x2019;m a pro at creating work for myself, so I wanted to do something extra.&lt;/p&gt;
&lt;h1 id=&#34;1-Planning-Begins&#34;&gt;&lt;a href=&#34;#1-Planning-Begins&#34; class=&#34;headerlink&#34; title=&#34;1. Planning Begins&#34;&gt;&lt;/a&gt;1. Planning Begins&lt;/h1&gt;&lt;p&gt;To give a quick update, I recently co-founded a startup called L0ch and Hacky-AI, where we&amp;#x2019;re developing security solutions. (If you&amp;#x2019;re interested, please visit our &lt;a href=&#34;https://hacky-ai.com/&#34;&gt;website&lt;/a&gt;!) I believe the most crucial aspect of running a startup is identifying customer problems and proposing solutions. There are plenty of other things to worry about too&amp;#x2026; Anyway, that&amp;#x2019;s why I thought, &amp;#x201C;Why not create something simple that people attending presentations need?&amp;#x201D;&lt;/p&gt;
&lt;p&gt;Since I&amp;#x2019;m also building solutions, I&amp;#x2019;ve become very interested in development using AI services. At this point, the things I wanted to stick to as much as possible were:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Don&amp;#x2019;t write the code myself; let the AI services write it all.&lt;/li&gt;
&lt;li&gt;We only have a week, so just make it work.&lt;/li&gt;
&lt;li&gt;Build a service people actually need.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Hacking Camp runs a CTF on the first evening. My assumption was that most participants were just starting out with hacking and probably weren&amp;#x2019;t very familiar with reverse engineering yet! So, wouldn&amp;#x2019;t it be great to develop a reverse engineering assistant tool? I figured I could probably build something like that by investing about three days on my own. (But this was arrogance.) More specifically, I thought I could use an LLM to rename variables and functions to more readable names, and even analyze function contents with an LLM to add comments. (It&amp;#x2019;s been over a year since I last opened IDA, so I realized while building this that such tools already exist as open source&amp;#x2026; haha. Well, the point was for me to try building a service anyway.)&lt;/p&gt;
&lt;p&gt;A simplified service flowchart would look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;hljs less&#34;&gt;&amp;#x250C;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2510;                &amp;#x250C;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2510;                &amp;#x250C;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2510;
&amp;#x2502;   &lt;span class=&#34;hljs-selector-tag&#34;&gt;IDA&lt;/span&gt; &lt;span class=&#34;hljs-selector-tag&#34;&gt;Pro&lt;/span&gt;       &amp;#x2502;&amp;#x25C4;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x25BA;&amp;#x2502;   &lt;span class=&#34;hljs-selector-tag&#34;&gt;FastAPI&lt;/span&gt;       &amp;#x2502;&amp;#x25C4;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x25BA;&amp;#x2502;   &lt;span class=&#34;hljs-selector-tag&#34;&gt;Ollama&lt;/span&gt;        &amp;#x2502;
&amp;#x2502;   &amp;#xD50C;&amp;#xB7EC;&amp;#xADF8;&amp;#xC778;        &amp;#x2502;                &amp;#x2502;   &amp;#xC11C;&amp;#xBC84;           &amp;#x2502;                &amp;#x2502;   &lt;span class=&#34;hljs-selector-tag&#34;&gt;LLM&lt;/span&gt; &amp;#xC11C;&amp;#xBC84;       &amp;#x2502;
&amp;#x2502; (Python)        &amp;#x2502;                &amp;#x2502; (Python)        &amp;#x2502;                &amp;#x2502;                 &amp;#x2502;
&amp;#x2514;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2518;                &amp;#x2514;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2518;                &amp;#x2514;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2518;
                                            &amp;#x2502;                                   &amp;#x2502;
                                            &amp;#x25BC;                                   &amp;#x25BC;
                                    &amp;#x250C;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2510;               &amp;#x250C;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2510; 
                                    &amp;#x2502;   &lt;span class=&#34;hljs-selector-tag&#34;&gt;Supabase&lt;/span&gt;      &amp;#x2502;               &amp;#x2502;   &lt;span class=&#34;hljs-selector-tag&#34;&gt;Langfuse&lt;/span&gt;      &amp;#x2502;
                                    &amp;#x2502;   &amp;#xB370;&amp;#xC774;&amp;#xD130;&amp;#xBCA0;&amp;#xC774;&amp;#xC2A4;     &amp;#x2502;               &amp;#x2502;   &amp;#xBAA8;&amp;#xB2C8;&amp;#xD130;&amp;#xB9C1;         &amp;#x2502;
                                    &amp;#x2514;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2518;               &amp;#x2514;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2500;&amp;#x2518;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;2-Ollama-Server&#34;&gt;&lt;a href=&#34;#2-Ollama-Server&#34; class=&#34;headerlink&#34; title=&#34;2. Ollama Server&#34;&gt;&lt;/a&gt;2. Ollama Server&lt;/h1&gt;&lt;p&gt;First, you might wonder why I&amp;#x2019;m not using a commercial service. After all, there are so many great models like Gemini and ChatGPT. My personal view is that &amp;#x201C;a small open-source model is sufficient for this service.&amp;#x201D; Before starting this venture, I used open-source models for projects at my previous company. Even now, while conducting research, I&amp;#x2019;ve tested various open-source models to build an on-premises environment. Unfortunately, performance limitations prevent me from testing larger models yet &amp;#x3160;&amp;#x3160;&lt;/p&gt;
&lt;p&gt;The device I currently use at home for LLM inference is a Mac mini. It has an M4 Pro chip and 64GB of RAM. I set up an Ollama server on it and served models. If you visit the &lt;a href=&#34;https://ollama.com/search&#34;&gt;Ollama homepage&lt;/a&gt;, you&amp;#x2019;ll find many models that can be utilized within 64GB of RAM.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/01.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-28 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 3.55.01.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Among the most popular models like GPT-OSS and DeepSeek, I tend to prefer models around 32B in size that are quantized to Q8. To give a simple reason:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;While 70B Q4 models (about 40G when downloaded) can run on 64GB RAM, these 70B models were already somewhat outdated at the time. I also thought it would be better to use a smaller model, considering the need to reserve space for the context window when processing long functions.&lt;/li&gt;
&lt;li&gt;Around mid-August, some hot new models emerged that were smaller yet performed well. For gpt-oss, there was a 20B MoE model, and qwen3 had 32B and 30B MoE models. These models seemed to deliver performance comparable to the latest ChatGPT 4o or slightly better.&lt;/li&gt;
&lt;li&gt;Beyond these, I also tested smaller models like Phi4 14B and Gemma 27B using several pre-set prompts I had prepared to evaluate their performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After multiple tests, I decided to use the &amp;#x201C;qwen3:30b-a3b-instruct-2507-q8_0&amp;#x201D; model.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;As expected, the MoE model is significantly faster at inference than the standard model. The Qwen3 32B q8 model achieved about 7 TPS, while the Qwen 30B MoE model reached up to 45 TPS. I felt this was sufficient for a small-scale service.&lt;/p&gt;
 &lt;pre&gt;&lt;code class=&#34;hljs yaml&#34;&gt;&lt;span class=&#34;hljs-attr&#34;&gt;total duration:&lt;/span&gt;       &lt;span class=&#34;hljs-number&#34;&gt;54.&lt;/span&gt;&lt;span class=&#34;hljs-string&#34;&gt;782917709s&lt;/span&gt;
&lt;span class=&#34;hljs-attr&#34;&gt;load duration:&lt;/span&gt;        &lt;span class=&#34;hljs-number&#34;&gt;83.&lt;/span&gt;&lt;span class=&#34;hljs-string&#34;&gt;491792ms&lt;/span&gt;
&lt;span class=&#34;hljs-attr&#34;&gt;prompt eval count:&lt;/span&gt;    &lt;span class=&#34;hljs-number&#34;&gt;2649 &lt;/span&gt;&lt;span class=&#34;hljs-string&#34;&gt;token(s)&lt;/span&gt;
&lt;span class=&#34;hljs-attr&#34;&gt;prompt eval duration:&lt;/span&gt; &lt;span class=&#34;hljs-number&#34;&gt;4.&lt;/span&gt;&lt;span class=&#34;hljs-string&#34;&gt;376536583s&lt;/span&gt;
&lt;span class=&#34;hljs-attr&#34;&gt;prompt eval rate:&lt;/span&gt;     &lt;span class=&#34;hljs-number&#34;&gt;605.27&lt;/span&gt; &lt;span class=&#34;hljs-string&#34;&gt;tokens/s&lt;/span&gt;
&lt;span class=&#34;hljs-attr&#34;&gt;eval count:&lt;/span&gt;           &lt;span class=&#34;hljs-number&#34;&gt;2293 &lt;/span&gt;&lt;span class=&#34;hljs-string&#34;&gt;token(s)&lt;/span&gt;
&lt;span class=&#34;hljs-attr&#34;&gt;eval duration:&lt;/span&gt;        &lt;span class=&#34;hljs-number&#34;&gt;49.&lt;/span&gt;&lt;span class=&#34;hljs-string&#34;&gt;986260996s&lt;/span&gt;
&lt;span class=&#34;hljs-attr&#34;&gt;eval rate:&lt;/span&gt;            &lt;span class=&#34;hljs-number&#34;&gt;45.87&lt;/span&gt; &lt;span class=&#34;hljs-string&#34;&gt;tokens/s&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The reason we did not use the reasoning model qwen3:30b-a3b-thinking-2507 was that a bug where reasoning failed to complete consistently appeared during prompt testing. Since the model had only recently been released at the time, we simply decided not to use it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;The reason for not using other Reasoning models was that they took significantly longer to complete outputs. While the instruct model could produce most outputs within 2 minutes, enabling Reasoning added an extra 1-2 minutes. Additionally, performance was sufficient even without Reasoning.&lt;/li&gt;
&lt;li&gt;Using smaller models required additional work, meaning their single-batch analysis performance was inferior. For the Phi4 14B model, while performance was sufficient, using the FP16 model was necessary to achieve usable performance levels. However, the reasoning time was too long for practical use. Beyond this, we determined that utilizing models like Qwen3 14B, 8B, Gemma 27B, and Deepseek 14B effectively would require significantly more prompt engineering effort.&lt;/li&gt;
&lt;li&gt;Considering various factors like speed, performance, and memory, we determined that using the &amp;#x201C;qwen3:30b-a3b-instruct-2507-q8_0&amp;#x201D; model was the most suitable choice.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With the model selected, simply set the environment variables and run the server using the &lt;code&gt;ollama serve&lt;/code&gt; command&amp;#x2014;that&amp;#x2019;s all it takes to get ready!&lt;/p&gt;
&lt;h1 id=&#34;2-Prompt-Logging-with-Langfuse&#34;&gt;&lt;a href=&#34;#2-Prompt-Logging-with-Langfuse&#34; class=&#34;headerlink&#34; title=&#34;2. Prompt Logging with Langfuse&#34;&gt;&lt;/a&gt;2. Prompt Logging with Langfuse&lt;/h1&gt;&lt;p&gt;You might be thinking, &amp;#x201C;Can&amp;#x2019;t we just use LangSmith?&amp;#x201D; I agree, but&amp;#x2026; since we tested it during the solution development process while considering on-premises deployment, I decided to give Langfuse a try this time.&lt;/p&gt;
&lt;p&gt;Installing Langfuse on the server isn&amp;#x2019;t difficult. Just grab it from the repository and deploy it via Docker. Then, register Langfuse&amp;#x2019;s callback handler in Langchain, and request logging will start as shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/02.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 4.22.19.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Logging allows you to verify whether responses to requests were sent correctly and check the quality of those responses. Based on this information, you can refine prompts or execution chains.&lt;/p&gt;
&lt;p&gt;And! You can also detect incoming attacks lol&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/03.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 4.24.45.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/04.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 4.24.58.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As shown above, we also detected prompt injection attacks being launched. You worked hard until dawn&amp;#x2026; Unfortunately, it seems you weren&amp;#x2019;t able to obtain the information you wanted.&lt;/p&gt;
&lt;h1 id=&#34;3-Lovable-Supabase&#34;&gt;&lt;a href=&#34;#3-Lovable-Supabase&#34; class=&#34;headerlink&#34; title=&#34;3. Lovable + Supabase&#34;&gt;&lt;/a&gt;3. Lovable + Supabase&lt;/h1&gt;&lt;p&gt;Originally, our website was developed using Base44. Yes, that&amp;#x2019;s the service &lt;a href=&#34;https://techcrunch.com/2025/06/18/6-month-old-solo-owned-vibe-coder-base44-sells-to-wix-for-80m-cash/&#34;&gt;acquired by Wix for $80M a few months ago&lt;/a&gt;. I tried it after reading this article too. It was fine for developing simple landing pages, but problems started piling up after adding login functionality. It was especially terrible when developing features that utilized the database.&lt;/p&gt;
&lt;p&gt;So, while searching for alternatives at the time, I heard that &amp;#x201C;Lovable is a hot service these days&amp;#x201D; and decided to give it a try. I first cloned the webpage I had previously developed exactly as it was, then started developing the login feature. To store user information, I connected it to Supabase. &lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/05.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 5.29.09.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Unlike Base44, it integrates directly with Supabase, makes database management easier, and seems to have many advantages overall. Of course, it might struggle to handle large-scale customers (my wallet, that is). Once you set up this integration and create the login button&amp;#x2026;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/06.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 5.36.58.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/07.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 5.37.39.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Yeah&amp;#x2026; it wasn&amp;#x2019;t exactly easy, but I did manage to succeed after a few tries. After logging in, there should be a dashboard, right? I&amp;#x2019;ll create it by writing the prompt well. &lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/08.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 5.43.31.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Of course, it doesn&amp;#x2019;t get cleanly developed with just that one prompt. If it had worked in one go, I&amp;#x2019;d be out buying Lovable stock right now.&lt;/p&gt;
&lt;p&gt;Anyway, there were still plenty of issues. While it generates API keys randomly just fine, it saves them to Supabase with parts of the key masked (&lt;em&gt;*&lt;/em&gt;), or arbitrarily removes buttons, and so on&amp;#x2014;there were a bunch of minor details to worry about. So I decided to narrow the scope of the features to implement. Originally, I wanted to add fun elements like a weekly Top 5 usage list&amp;#x2026; but it required more prompting than expected, so I decided to skip it for now.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2025/10/30/j0ker/service_dev_part1/en/09.png&#34; alt=&#34;&amp;#x1109;&amp;#x1173;&amp;#x110F;&amp;#x1173;&amp;#x1105;&amp;#x1175;&amp;#x11AB;&amp;#x1109;&amp;#x1163;&amp;#x11BA; 2025-10-29 &amp;#x110B;&amp;#x1169;&amp;#x1112;&amp;#x116E; 6.55.28.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;After about four hours of hard work, I managed to implement all the essential features. If I had developed it myself with my limited web development skills, it would have taken a week. Using AI services these days is definitely more advantageous.&lt;/p&gt;
&lt;p&gt;I&amp;#x2019;ll wrap things up here for today. Next time, I&amp;#x2019;ll cover the rest of the development process and what happened at the presentation site. Until then&amp;#x2026; wish me luck staying alive&amp;#x2026; Bye!&lt;/p&gt;
 - hack &amp; life">
  <meta property="og:image" content="https://hackyboiz.github.io/2025/10/30/j0ker/service_dev_part1/en/thumbnail.jpg">
  <meta property="og:url" content="https://hackyboiz.github.io/">

  <link rel="canonical" href="https://hackyboiz.github.io/2025/10/30/j0ker/service_dev_part1/en/">

  <title>[Research] Newbie Developer&#39;s Service Development Journey using AI Services Part 1 - hackyboiz</title>

  <link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css">


  <link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css">
  <link rel="stylesheet" href="/lib/hint/hint.min.css">

  
    
    <link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css">
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_6peoq002giu.css">
<link rel="stylesheet" href="/.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">
<link rel="stylesheet" href="/.css">


<link rel="stylesheet" href="/css/main.css">

<!-- 自定义样式保持在最底部 -->


  <script src="/js/utils.js"></script>
  <script src="/js/color-schema.js"></script>

<meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/rss2.xml" title="hackyboiz" type="application/rss+xml">
</head>


<body>
  <header style="height: 40vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">&nbsp;<strong>Hackyboiz</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-addrcard"></i>
                About
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/author/">
                <i class="iconfont icon-user-fill"></i>
                Author
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax="true" style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container page-header text-center fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2025-10-30 18:00" pubdate>
      2025년 10월 30일 오후
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      1.9k 자
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      37
       분
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2">
      <!--<script data-ad-client="ca-pub-3672652207808168" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>-->
    </div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto" id="post">
            <!-- SEO header -->
            <h1 style="display: none">[Research] Newbie Developer&#39;s Service Development Journey using AI Services Part 1</h1>
                       
            <div class="markdown-body" id="post-body">
              <!-- <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>-->
              <!-- hackyboiz_horizen_index -->
              <!--
              <ins class="adsbygoogle"
                   style="display:block"
                   data-ad-client="ca-pub-3672652207808168"
                   data-ad-slot="8887862313"
                   data-ad-format="auto"
                   data-full-width-responsive="true"></ins>
              <script>
                   (adsbygoogle = window.adsbygoogle || []).push({});
              </script>
              -->
              <p><img src="/2025/10/30/j0ker/service_dev_part1/en/thumbnail.jpg" srcset="/img/loading.gif" alt="index_img"></p>
<h1 id="0-What-happened&#x2026;"><a href="#0-What-happened&#x2026;" class="headerlink" title="0. What happened&#x2026;"></a>0. What happened&#x2026;</h1><p>Haha, it&#x2019;s j0ker! Why&#x2019;s it been so long since I last wrote? When was my last post&#x2026; Ah, August&#x2026; Haha&#x2026; Please save me&#x2026; Next time I&#x2019;ll try to write something more interesting&#x2026;</p>
<p><img src="/2025/10/30/j0ker/service_dev_part1/en/joker.png" srcset="/img/loading.gif" alt="image.png"></p>
<p>Today, unlike my previous research posts, I&#x2019;d like to briefly discuss a project I developed.</p>
<p>On August 30th, the 31st Hacking Camp took place. About two weeks prior, I suddenly received a call from the organizers&#x2026;</p>
<p><img src="/2025/10/30/j0ker/service_dev_part1/en/kakao.png" srcset="/img/loading.gif" alt="&#x1112;&#x1162;&#x110F;&#x1175;&#x11BC;&#x110F;&#x1162;&#x11B7;&#x1111;&#x1173; &#x1109;&#x1165;&#x11B8;&#x110B;&#x116C;.png"></p>
<p>Well&#x2026; the original presenter suddenly contacted us saying they couldn&#x2019;t make it, so we had to find a replacement. I was planning to go on the second day, listen to the mentee presentations, and then grab drinks with the organizers in the evening&#x2026; but I ended up giving the presentation lol. So I prepared for it and had about a week to spare&#x2026; but I&#x2019;m a pro at creating work for myself, so I wanted to do something extra.</p>
<h1 id="1-Planning-Begins"><a href="#1-Planning-Begins" class="headerlink" title="1. Planning Begins"></a>1. Planning Begins</h1><p>To give a quick update, I recently co-founded a startup called L0ch and Hacky-AI, where we&#x2019;re developing security solutions. (If you&#x2019;re interested, please visit our <a target="_blank" rel="external nofollow noopener noreferrer" href="https://hacky-ai.com/">website</a>!) I believe the most crucial aspect of running a startup is identifying customer problems and proposing solutions. There are plenty of other things to worry about too&#x2026; Anyway, that&#x2019;s why I thought, &#x201C;Why not create something simple that people attending presentations need?&#x201D;</p>
<p>Since I&#x2019;m also building solutions, I&#x2019;ve become very interested in development using AI services. At this point, the things I wanted to stick to as much as possible were:</p>
<ol>
<li>Don&#x2019;t write the code myself; let the AI services write it all.</li>
<li>We only have a week, so just make it work.</li>
<li>Build a service people actually need.</li>
</ol>
<p>Hacking Camp runs a CTF on the first evening. My assumption was that most participants were just starting out with hacking and probably weren&#x2019;t very familiar with reverse engineering yet! So, wouldn&#x2019;t it be great to develop a reverse engineering assistant tool? I figured I could probably build something like that by investing about three days on my own. (But this was arrogance.) More specifically, I thought I could use an LLM to rename variables and functions to more readable names, and even analyze function contents with an LLM to add comments. (It&#x2019;s been over a year since I last opened IDA, so I realized while building this that such tools already exist as open source&#x2026; haha. Well, the point was for me to try building a service anyway.)</p>
<p>A simplified service flowchart would look like this:</p>
<pre><code class="hljs less">&#x250C;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2510;                &#x250C;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2510;                &#x250C;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2510;
&#x2502;   <span class="hljs-selector-tag">IDA</span> <span class="hljs-selector-tag">Pro</span>       &#x2502;&#x25C4;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x25BA;&#x2502;   <span class="hljs-selector-tag">FastAPI</span>       &#x2502;&#x25C4;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x25BA;&#x2502;   <span class="hljs-selector-tag">Ollama</span>        &#x2502;
&#x2502;   &#xD50C;&#xB7EC;&#xADF8;&#xC778;        &#x2502;                &#x2502;   &#xC11C;&#xBC84;           &#x2502;                &#x2502;   <span class="hljs-selector-tag">LLM</span> &#xC11C;&#xBC84;       &#x2502;
&#x2502; (Python)        &#x2502;                &#x2502; (Python)        &#x2502;                &#x2502;                 &#x2502;
&#x2514;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2518;                &#x2514;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2518;                &#x2514;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2518;
                                            &#x2502;                                   &#x2502;
                                            &#x25BC;                                   &#x25BC;
                                    &#x250C;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2510;               &#x250C;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2510; 
                                    &#x2502;   <span class="hljs-selector-tag">Supabase</span>      &#x2502;               &#x2502;   <span class="hljs-selector-tag">Langfuse</span>      &#x2502;
                                    &#x2502;   &#xB370;&#xC774;&#xD130;&#xBCA0;&#xC774;&#xC2A4;     &#x2502;               &#x2502;   &#xBAA8;&#xB2C8;&#xD130;&#xB9C1;         &#x2502;
                                    &#x2514;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2518;               &#x2514;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2518;
</code></pre>
<h1 id="2-Ollama-Server"><a href="#2-Ollama-Server" class="headerlink" title="2. Ollama Server"></a>2. Ollama Server</h1><p>First, you might wonder why I&#x2019;m not using a commercial service. After all, there are so many great models like Gemini and ChatGPT. My personal view is that &#x201C;a small open-source model is sufficient for this service.&#x201D; Before starting this venture, I used open-source models for projects at my previous company. Even now, while conducting research, I&#x2019;ve tested various open-source models to build an on-premises environment. Unfortunately, performance limitations prevent me from testing larger models yet &#x3160;&#x3160;</p>
<p>The device I currently use at home for LLM inference is a Mac mini. It has an M4 Pro chip and 64GB of RAM. I set up an Ollama server on it and served models. If you visit the <a target="_blank" rel="external nofollow noopener noreferrer" href="https://ollama.com/search">Ollama homepage</a>, you&#x2019;ll find many models that can be utilized within 64GB of RAM.</p>
<p><img src="/2025/10/30/j0ker/service_dev_part1/en/01.png" srcset="/img/loading.gif" alt="&#x1109;&#x1173;&#x110F;&#x1173;&#x1105;&#x1175;&#x11AB;&#x1109;&#x1163;&#x11BA; 2025-10-28 &#x110B;&#x1169;&#x1112;&#x116E; 3.55.01.png"></p>
<p>Among the most popular models like GPT-OSS and DeepSeek, I tend to prefer models around 32B in size that are quantized to Q8. To give a simple reason:</p>
<ul>
<li>While 70B Q4 models (about 40G when downloaded) can run on 64GB RAM, these 70B models were already somewhat outdated at the time. I also thought it would be better to use a smaller model, considering the need to reserve space for the context window when processing long functions.</li>
<li>Around mid-August, some hot new models emerged that were smaller yet performed well. For gpt-oss, there was a 20B MoE model, and qwen3 had 32B and 30B MoE models. These models seemed to deliver performance comparable to the latest ChatGPT 4o or slightly better.</li>
<li>Beyond these, I also tested smaller models like Phi4 14B and Gemma 27B using several pre-set prompts I had prepared to evaluate their performance.</li>
</ul>
<p>After multiple tests, I decided to use the &#x201C;qwen3:30b-a3b-instruct-2507-q8_0&#x201D; model.</p>
<ol>
<li><p>As expected, the MoE model is significantly faster at inference than the standard model. The Qwen3 32B q8 model achieved about 7 TPS, while the Qwen 30B MoE model reached up to 45 TPS. I felt this was sufficient for a small-scale service.</p>
 <pre><code class="hljs yaml"><span class="hljs-attr">total duration:</span>       <span class="hljs-number">54.</span><span class="hljs-string">782917709s</span>
<span class="hljs-attr">load duration:</span>        <span class="hljs-number">83.</span><span class="hljs-string">491792ms</span>
<span class="hljs-attr">prompt eval count:</span>    <span class="hljs-number">2649 </span><span class="hljs-string">token(s)</span>
<span class="hljs-attr">prompt eval duration:</span> <span class="hljs-number">4.</span><span class="hljs-string">376536583s</span>
<span class="hljs-attr">prompt eval rate:</span>     <span class="hljs-number">605.27</span> <span class="hljs-string">tokens/s</span>
<span class="hljs-attr">eval count:</span>           <span class="hljs-number">2293 </span><span class="hljs-string">token(s)</span>
<span class="hljs-attr">eval duration:</span>        <span class="hljs-number">49.</span><span class="hljs-string">986260996s</span>
<span class="hljs-attr">eval rate:</span>            <span class="hljs-number">45.87</span> <span class="hljs-string">tokens/s</span></code></pre>
</li>
<li><p>The reason we did not use the reasoning model qwen3:30b-a3b-thinking-2507 was that a bug where reasoning failed to complete consistently appeared during prompt testing. Since the model had only recently been released at the time, we simply decided not to use it.</p>
</li>
<li>The reason for not using other Reasoning models was that they took significantly longer to complete outputs. While the instruct model could produce most outputs within 2 minutes, enabling Reasoning added an extra 1-2 minutes. Additionally, performance was sufficient even without Reasoning.</li>
<li>Using smaller models required additional work, meaning their single-batch analysis performance was inferior. For the Phi4 14B model, while performance was sufficient, using the FP16 model was necessary to achieve usable performance levels. However, the reasoning time was too long for practical use. Beyond this, we determined that utilizing models like Qwen3 14B, 8B, Gemma 27B, and Deepseek 14B effectively would require significantly more prompt engineering effort.</li>
<li>Considering various factors like speed, performance, and memory, we determined that using the &#x201C;qwen3:30b-a3b-instruct-2507-q8_0&#x201D; model was the most suitable choice.</li>
</ol>
<p>With the model selected, simply set the environment variables and run the server using the <code>ollama serve</code> command&#x2014;that&#x2019;s all it takes to get ready!</p>
<h1 id="2-Prompt-Logging-with-Langfuse"><a href="#2-Prompt-Logging-with-Langfuse" class="headerlink" title="2. Prompt Logging with Langfuse"></a>2. Prompt Logging with Langfuse</h1><p>You might be thinking, &#x201C;Can&#x2019;t we just use LangSmith?&#x201D; I agree, but&#x2026; since we tested it during the solution development process while considering on-premises deployment, I decided to give Langfuse a try this time.</p>
<p>Installing Langfuse on the server isn&#x2019;t difficult. Just grab it from the repository and deploy it via Docker. Then, register Langfuse&#x2019;s callback handler in Langchain, and request logging will start as shown below.</p>
<p><img src="/2025/10/30/j0ker/service_dev_part1/en/02.png" srcset="/img/loading.gif" alt="&#x1109;&#x1173;&#x110F;&#x1173;&#x1105;&#x1175;&#x11AB;&#x1109;&#x1163;&#x11BA; 2025-10-29 &#x110B;&#x1169;&#x1112;&#x116E; 4.22.19.png"></p>
<p>Logging allows you to verify whether responses to requests were sent correctly and check the quality of those responses. Based on this information, you can refine prompts or execution chains.</p>
<p>And! You can also detect incoming attacks lol</p>
<p><img src="/2025/10/30/j0ker/service_dev_part1/en/03.png" srcset="/img/loading.gif" alt="&#x1109;&#x1173;&#x110F;&#x1173;&#x1105;&#x1175;&#x11AB;&#x1109;&#x1163;&#x11BA; 2025-10-29 &#x110B;&#x1169;&#x1112;&#x116E; 4.24.45.png"></p>
<p><img src="/2025/10/30/j0ker/service_dev_part1/en/04.png" srcset="/img/loading.gif" alt="&#x1109;&#x1173;&#x110F;&#x1173;&#x1105;&#x1175;&#x11AB;&#x1109;&#x1163;&#x11BA; 2025-10-29 &#x110B;&#x1169;&#x1112;&#x116E; 4.24.58.png"></p>
<p>As shown above, we also detected prompt injection attacks being launched. You worked hard until dawn&#x2026; Unfortunately, it seems you weren&#x2019;t able to obtain the information you wanted.</p>
<h1 id="3-Lovable-Supabase"><a href="#3-Lovable-Supabase" class="headerlink" title="3. Lovable + Supabase"></a>3. Lovable + Supabase</h1><p>Originally, our website was developed using Base44. Yes, that&#x2019;s the service <a target="_blank" rel="external nofollow noopener noreferrer" href="https://techcrunch.com/2025/06/18/6-month-old-solo-owned-vibe-coder-base44-sells-to-wix-for-80m-cash/">acquired by Wix for $80M a few months ago</a>. I tried it after reading this article too. It was fine for developing simple landing pages, but problems started piling up after adding login functionality. It was especially terrible when developing features that utilized the database.</p>
<p>So, while searching for alternatives at the time, I heard that &#x201C;Lovable is a hot service these days&#x201D; and decided to give it a try. I first cloned the webpage I had previously developed exactly as it was, then started developing the login feature. To store user information, I connected it to Supabase. </p>
<p><img src="/2025/10/30/j0ker/service_dev_part1/en/05.png" srcset="/img/loading.gif" alt="&#x1109;&#x1173;&#x110F;&#x1173;&#x1105;&#x1175;&#x11AB;&#x1109;&#x1163;&#x11BA; 2025-10-29 &#x110B;&#x1169;&#x1112;&#x116E; 5.29.09.png"></p>
<p>Unlike Base44, it integrates directly with Supabase, makes database management easier, and seems to have many advantages overall. Of course, it might struggle to handle large-scale customers (my wallet, that is). Once you set up this integration and create the login button&#x2026;</p>
<p><img src="/2025/10/30/j0ker/service_dev_part1/en/06.png" srcset="/img/loading.gif" alt="&#x1109;&#x1173;&#x110F;&#x1173;&#x1105;&#x1175;&#x11AB;&#x1109;&#x1163;&#x11BA; 2025-10-29 &#x110B;&#x1169;&#x1112;&#x116E; 5.36.58.png"></p>
<p><img src="/2025/10/30/j0ker/service_dev_part1/en/07.png" srcset="/img/loading.gif" alt="&#x1109;&#x1173;&#x110F;&#x1173;&#x1105;&#x1175;&#x11AB;&#x1109;&#x1163;&#x11BA; 2025-10-29 &#x110B;&#x1169;&#x1112;&#x116E; 5.37.39.png"></p>
<p>Yeah&#x2026; it wasn&#x2019;t exactly easy, but I did manage to succeed after a few tries. After logging in, there should be a dashboard, right? I&#x2019;ll create it by writing the prompt well. </p>
<p><img src="/2025/10/30/j0ker/service_dev_part1/en/08.png" srcset="/img/loading.gif" alt="&#x1109;&#x1173;&#x110F;&#x1173;&#x1105;&#x1175;&#x11AB;&#x1109;&#x1163;&#x11BA; 2025-10-29 &#x110B;&#x1169;&#x1112;&#x116E; 5.43.31.png"></p>
<p>Of course, it doesn&#x2019;t get cleanly developed with just that one prompt. If it had worked in one go, I&#x2019;d be out buying Lovable stock right now.</p>
<p>Anyway, there were still plenty of issues. While it generates API keys randomly just fine, it saves them to Supabase with parts of the key masked (<em>*</em>), or arbitrarily removes buttons, and so on&#x2014;there were a bunch of minor details to worry about. So I decided to narrow the scope of the features to implement. Originally, I wanted to add fun elements like a weekly Top 5 usage list&#x2026; but it required more prompting than expected, so I decided to skip it for now.</p>
<p><img src="/2025/10/30/j0ker/service_dev_part1/en/09.png" srcset="/img/loading.gif" alt="&#x1109;&#x1173;&#x110F;&#x1173;&#x1105;&#x1175;&#x11AB;&#x1109;&#x1163;&#x11BA; 2025-10-29 &#x110B;&#x1169;&#x1112;&#x116E; 6.55.28.png"></p>
<p>After about four hours of hard work, I managed to implement all the essential features. If I had developed it myself with my limited web development skills, it would have taken a week. Using AI services these days is definitely more advantageous.</p>
<p>I&#x2019;ll wrap things up here for today. Next time, I&#x2019;ll cover the rest of the development process and what happened at the presentation site. Until then&#x2026; wish me luck staying alive&#x2026; Bye!</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                <div class="post-meta mr-3">
                  <i class="iconfont icon-category"></i>
                  
                  <a class="hover-with-bg" href="/categories/Research/">Research</a>
                  
                </div>
                
                
                <div class="post-meta">
                  <i class="iconfont icon-tags"></i>
                  
                  <a class="hover-with-bg" href="/tags/j0ker/">j0ker</a>
                  
                  <a class="hover-with-bg" href="/tags/ollama/">ollama</a>
                  
                  <a class="hover-with-bg" href="/tags/llm/">llm</a>
                  
                  <a class="hover-with-bg" href="/tags/plugin/">plugin</a>
                  
                  <a class="hover-with-bg" href="/tags/startup/">startup</a>
                  
                  <a class="hover-with-bg" href="/tags/develope/">develope</a>
                  
                  <a class="hover-with-bg" href="/tags/ida/">ida</a>
                  
                  <a class="hover-with-bg" href="/tags/hacky-ai/">hacky-ai</a>
                  
                  <a class="hover-with-bg" href="/tags/lovable/">lovable</a>
                  
                  <a class="hover-with-bg" href="/tags/supabse/">supabse</a>
                  
                  <a class="hover-with-bg" href="/tags/qwen/">qwen</a>
                  
                  <a class="hover-with-bg" href="/tags/langfuse/">langfuse</a>
                  
                </div>
                
              </div>

              <div class="post-metas mb-3">
                <a class="hover-with-bg" style="display: flex;" href="/author">
                  <div class="link-avatar-page">
                    <img src="/img/profile_j0ker.jpg" srcset="/img/loading.gif" alt="j0ker">
                  </div>

                  <div class="link-text">
                    <div class="link-title">j0ker</div>
                  </div>
                </a>
                <div class="link-text">
                  <div class="link-more">
                    <a href="/tags/j0ker">
                      Read more
                      <i class="iconfont icon-arrowdown"></i>
                    </a>
                  </div>
                </div>
              </div>

              <hr>
              <!--<script data-ad-client="ca-pub-3672652207808168" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>-->
              
              <!--  -->
              <p class="note note-warning">본 글은 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.ko" rel="external nofollow noopener noreferrer">CC BY-SA 4.0</a> 라이선스로 배포됩니다. 공유 또는 변경 시 반드시 출처를 남겨주시기 바랍니다.</p>
              
              
              
              <div class="post-prevnext row">
                <article class="post-prev col-6">
                  
                  
                </article>
                <article class="post-next col-6">
                  
                  
                  <a href="/2025/10/30/j0ker/service_dev_part1/kr/">
                    <span class="hidden-mobile">[Research] 초짜 개발자의 AI 서비스 삽질기 Part 1</span>
                    <span class="visible-mobile">Next</span>
                    <i class="iconfont icon-arrowright"></i>
                  </a>
                  
                </article>
              </div>
              
            </div>
            
            <!-- Embed Section -->
            <div style="width: 100%; height: 210px; margin-bottom: 50px; overflow: hidden;">
              <iframe src="https://maily.so/hackyboiz/embed?src=embed" style="width: 100%; height: 100%; border: none;" frameborder="0"></iframe>
            </div>            

            
            <!-- Comments -->
            <article class="comments" id="comments">
              
              
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_config = function () {
        this.page.url = 'https://hackyboiz.github.io/2025/10/30/j0ker/service_dev_part1/en/';
        this.page.identifier = '/2025/10/30/j0ker/service_dev_part1/en/';
      };
      function loadDisqus() {
        (function () {
          var d = document,
            s = d.createElement('script');
          s.src = '//' + 'hackyboiz' + '.disqus.com/embed.js';
          s.setAttribute('data-timestamp', new Date());
          (d.head || d.body).appendChild(s);
        })();
      }
      waitElementVisible('disqus_thread', loadDisqus);
    </script>
    <noscript>Please enable JavaScript to view the
      <a target="_blank" href="https://disqus.com/?ref_noscript" rel="external nofollow noopener noreferrer">comments powered by Disqus.</a>
    </noscript>
  </div>


            </article>
            
          </article>
        </div>
      </div>
    </div>
    
    <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
      <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>

    </div>
    
  </div>
</div>

<!-- Custom -->

    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="external nofollow noopener noreferrer"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="external nofollow noopener noreferrer">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js"></script>
<script src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js"></script>
<script src="/js/debouncer.js"></script>
<script src="/js/main.js"></script>

<!-- Plugins -->


  
    <script src="/js/lazyload.js"></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js"></script>
  <script src="/js/clipboard-use.js"></script>







  <script src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js"></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '#post-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js"></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "[Research] Newbie Developer's Service Development Journey using AI Services Part 1&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js"></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js"></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  <link rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css">

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js"></script>

  













  

  
    <!-- Google Analytics -->
    <script defer>
      window.ga = window.ga || function () { (ga.q = ga.q || []).push(arguments) };
      ga.l = +new Date;
      ga('create', 'UA-177243668-2', 'auto');
      ga('send', 'pageview');
    </script>
    <script async src="https://www.google-analytics.com/analytics.js"></script>
  

  
    <!-- Google gtag.js -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-177243668-2"></script>
    <script defer>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-177243668-2');
    </script>
  

  

  

  





</body>
</html>
